{"cells":[{"cell_type":"markdown","metadata":{},"source":["\n","<div style=\"text-align: center;\">\n","    <h1>ReConPatch: anomaly detection</h1>\n","    <h3>Authors:</h3>\n","    <p>Dario Loi 1940849, Elena Muia 1938610, Martina Doku 1938629</p>\n","\n","</div>\n"]},{"cell_type":"markdown","metadata":{},"source":["<div>\n","    <h2>0 - Introduction</h2>\n","    <p>This project aims to reimplement and potentially advance the ReConPatch method proposed in the paper titled  <a href=\"https://arxiv.org/pdf/2305.16713v3\">”ReConPatch:\n","Anomaly Detection by Linear Modulation of Pretrained Features.”</a> This method addresses the challenge of\n","anomaly detection by constructing discriminative features through a linear modulation of patch features extracted\n","from pre-trained models and employs contrastive representation learning to collect and distribute features in a way\n","that produces a target-oriented and easily separable representation of the data.</p>\n","</div>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T15:39:23.353383Z","iopub.status.busy":"2024-05-30T15:39:23.352483Z","iopub.status.idle":"2024-05-30T15:39:39.753083Z","shell.execute_reply":"2024-05-30T15:39:39.751760Z","shell.execute_reply.started":"2024-05-30T15:39:23.353347Z"},"trusted":true},"outputs":[],"source":["pip install ema-pytorch"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T15:39:39.755544Z","iopub.status.busy":"2024-05-30T15:39:39.755232Z","iopub.status.idle":"2024-05-30T15:39:57.217126Z","shell.execute_reply":"2024-05-30T15:39:57.215838Z","shell.execute_reply.started":"2024-05-30T15:39:39.755515Z"},"trusted":true},"outputs":[],"source":["pip install lightning"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T15:39:57.219389Z","iopub.status.busy":"2024-05-30T15:39:57.218957Z","iopub.status.idle":"2024-05-30T15:40:15.025839Z","shell.execute_reply":"2024-05-30T15:40:15.024480Z","shell.execute_reply.started":"2024-05-30T15:39:57.219345Z"},"trusted":true},"outputs":[],"source":["pip install adamp"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T15:40:15.028849Z","iopub.status.busy":"2024-05-30T15:40:15.028480Z","iopub.status.idle":"2024-05-30T15:40:26.282675Z","shell.execute_reply":"2024-05-30T15:40:26.281802Z","shell.execute_reply.started":"2024-05-30T15:40:15.028815Z"},"trusted":true},"outputs":[],"source":["import os\n","from typing import Optional\n","from lightning import LightningDataModule, LightningModule\n","import lightning.pytorch as pl\n","import torch.optim as optim\n","import torch\n","from torchvision import transforms\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","from tqdm import tqdm\n","from ema_pytorch import EMA\n","import wandb\n","from lightning.pytorch.loggers import WandbLogger\n","from lightning.pytorch.callbacks import ModelCheckpoint\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","from adamp import AdamP\n","import lightning.pytorch as torchpl\n","from torchvision.transforms import transforms\n","from sklearn import random_projection\n","import numpy as np"]},{"cell_type":"markdown","metadata":{},"source":["<div>\n","    <h2>1.1 - Data</h2>\n","    <p>In this study, we used the <a href=\"https://arxiv.org/pdf/2305.16713v3\">MVTec AD</a> dataset\n","and <a href=\"https://arxiv.org/pdf/2305.16713v3\">BTAD</a> dataset for our experiments</p>\n","</div>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T15:40:26.284273Z","iopub.status.busy":"2024-05-30T15:40:26.283822Z","iopub.status.idle":"2024-05-30T15:40:26.288938Z","shell.execute_reply":"2024-05-30T15:40:26.288005Z","shell.execute_reply.started":"2024-05-30T15:40:26.284239Z"},"trusted":true},"outputs":[],"source":["#data\n","#downloaded from https://www.kaggle.com/uciml/pima-indians-diabetes-database\n","#!wget https://www.mvtec.com/company/research/datasets/mvtec-ad/downloads/mvtec_anomaly_detection.tar.xz\n"]},{"cell_type":"markdown","metadata":{},"source":["<div>\n","    <h2>1.2 - Data preprocessing</h2>\n","    <p>We create a unique data module to feed to the lightning module for both the datasets. The preprocessing will follow the ones specified in the sections 4.3 and 4.4 of the aforementioned paper.</p>\n","</div>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T17:28:05.307236Z","iopub.status.busy":"2024-05-30T17:28:05.306818Z","iopub.status.idle":"2024-05-30T17:28:05.334943Z","shell.execute_reply":"2024-05-30T17:28:05.333820Z","shell.execute_reply.started":"2024-05-30T17:28:05.307202Z"},"trusted":true},"outputs":[],"source":["\n","class MVTecDataModule(LightningDataModule):\n","    def __init__(\n","        self,\n","        data_dir: str,\n","        batch_size: int = 16,\n","        num_workers: int = 4,\n","        train_val_split: float = 0.8,\n","        shuffle: bool = True,\n","        pin_memory: bool = True,\n","        image_size: int = 256,\n","        normalize: Optional[transforms.Normalize] = None,\n","        **kwargs,\n","    ):\n","        super().__init__()\n","        self.data_dir = data_dir\n","        self.batch_size = batch_size\n","        self.num_workers = num_workers\n","        self.train_val_split = train_val_split\n","        self.shuffle = shuffle\n","        self.pin_memory = pin_memory\n","        self.image_size = image_size\n","        self.normalize = normalize\n","\n","    def setup(self, stage=None):\n","        # Define transformations\n","        transform = transforms.Compose([\n","            transforms.Resize((self.image_size, self.image_size)),\n","            transforms.ToTensor(),\n","            transforms.CenterCrop(size=(224, 224)),\n","            self.normalize if self.normalize else transforms.Lambda(lambda x: x),\n","        ])\n","        transform_test = transforms.Compose([\n","            transforms.Resize((self.image_size, self.image_size)),\n","            transforms.ToTensor(),\n","            transforms.CenterCrop(size=(224, 224)),\n","        ]\n","        )\n","\n","        # Load dataset\n","        tot_num=0\n","        for subclass in [\"bottle\",\"cable\",\"capsule\",\"carpet\",\"grid\",\"hazelnut\",\"leather\",\"metal_nut\",\"pill\",\"screw\",\"tile\",\"toothbrush\",\"transistor\",\"wood\",\"zipper\"]:\n","            dataset = ImageFolder(os.path.join(self.data_dir, subclass,\"train\"), transform=transform)\n","            test_dataset = ImageFolder(os.path.join(self.data_dir, subclass,\"test\"), transform=transform)\n","            # Split dataset into train and validation sets\n","            num_train = int(len(dataset) * self.train_val_split)\n","            num_val = len(dataset) - num_train\n","            tot_num+=len(dataset)\n","            train_dataset, val_dataset = torch.utils.data.random_split(\n","                dataset, [num_train, num_val])\n","            #print(subclass)\n","            #print('num_train',num_train)\n","            #print('num_val',num_val)\n","            #add the subclass to the dataset\n","            if subclass == \"bottle\":\n","                self.train_dataset = train_dataset\n","                self.val_dataset = val_dataset\n","                self.test_dataset = test_dataset\n","            else:\n","                self.train_dataset = torch.utils.data.ConcatDataset([self.train_dataset,train_dataset])\n","                self.val_dataset = torch.utils.data.ConcatDataset([self.val_dataset,val_dataset])\n","                self.test_dataset = torch.utils.data.ConcatDataset([self.test_dataset,test_dataset])\n","\n","        #print the number of images in the dataset\n","        print(\"number of images in train dataset\",len(self.train_dataset))\n","        print(\"number of images in val dataset\",len(self.val_dataset))\n","        print(\"number of images in test dataset\",len(self.test_dataset))\n","\n","\n","    def train_dataloader(self):\n","        return DataLoader(\n","            self.train_dataset,\n","            batch_size=self.batch_size,\n","            shuffle=self.shuffle,\n","            num_workers=self.num_workers,\n","            pin_memory=self.pin_memory,\n","        )\n","\n","    def val_dataloader(self):\n","        return DataLoader(\n","            self.val_dataset,\n","            batch_size=self.batch_size,\n","            shuffle=False,\n","            num_workers=self.num_workers,\n","            pin_memory=self.pin_memory,\n","        )\n","\n","    def test_dataloader(self):\n","#         # Load test dataset without shuffling\n","#         transform = transforms.Compose([\n","#             transforms.Resize((self.image_size, self.image_size)),\n","#             transforms.ToTensor(),\n","#             transforms.CenterCrop(size=(224, 224)),\n","#             self.normalize if self.normalize else transforms.Lambda(lambda x: x),\n","#         ])\n","#         for subclass in [\"bottle\",\"cable\",\"capsule\",\"carpet\",\"grid\",\"hazelnut\",\"leather\",\"metal_nut\",\"pill\",\"screw\",\"tile\",\"toothbrush\",\"transistor\",\"wood\",\"zipper\"]:\n","#             test_dataset = ImageFolder(os.path.join(self.data_dir, subclass,\"test\"), transform=transform)\n","#             if subclass == \"bottle\":\n","#                 self.test_dataset = test_dataset\n","#             else:\n","#                 self.test_dataset = torch.utils.data.ConcatDataset([self.test_dataset,test_dataset])\n","\n","        return DataLoader(\n","            self.test_dataset,\n","            batch_size=self.batch_size,\n","            shuffle=False,\n","            num_workers=self.num_workers,\n","            pin_memory=self.pin_memory,\n","        )\n","    def show_examples(self, dataloader, title):\n","        \n","        # Get a batch of samples\n","        images, labels = next(iter(dataloader))\n","        # Select one sample from the batch\n","        image = images[0]\n","        # Convert the tensor to a numpy array for displaying\n","        image = image.permute(1, 2, 0).numpy()\n","        plt.imshow(image)\n","        plt.title(f'{title}')\n","        plt.show()\n","\n","    def show_random_images(self, dataloader, title, num_images=10):\n","        fig, axs = plt.subplots(2, num_images // 2, figsize=(15, 8))\n","        axs = axs.flatten()\n","\n","        # Randomly select images from the dataloader\n","        selected_indices = random.sample(range(len(dataloader.dataset)), num_images)\n","        \n","        for i, idx in enumerate(selected_indices):\n","            image, mask = dataloader.dataset[idx]\n","            image = image.permute(1, 2, 0).numpy()\n","\n","            axs[i].imshow(image)\n","            axs[i].set_title(f'{title} - Image {i+1}')\n","            axs[i].axis('off')\n","\n","        plt.tight_layout()\n","        plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T17:28:08.305693Z","iopub.status.busy":"2024-05-30T17:28:08.305261Z","iopub.status.idle":"2024-05-30T17:28:08.430169Z","shell.execute_reply":"2024-05-30T17:28:08.429162Z","shell.execute_reply.started":"2024-05-30T17:28:08.305660Z"},"trusted":true},"outputs":[],"source":["\n","import matplotlib.pyplot as plt\n","# Example usage\n","data_module = MVTecDataModule(data_dir='/kaggle/input/mvtec-ad')\n","data_module.setup()\n","train_loader = data_module.train_dataloader()\n","val_loader = data_module.val_dataloader()\n","test_loader = data_module.test_dataloader()\n","\n","'''\n","# Show examples\n","data_module.show_examples(train_loader, 'Training Example')\n","data_module.show_examples(val_loader, 'Validation Example')\n","data_module.show_examples(test_loader, 'Test Example')\n","'''\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T17:28:30.425170Z","iopub.status.busy":"2024-05-30T17:28:30.424733Z","iopub.status.idle":"2024-05-30T17:28:36.585037Z","shell.execute_reply":"2024-05-30T17:28:36.583961Z","shell.execute_reply.started":"2024-05-30T17:28:30.425137Z"},"trusted":true},"outputs":[],"source":["import random\n","# Show random images from the training dataloader\n","data_module.show_random_images(train_loader, 'Training Random Images', num_images=8)\n","\n","# Show random images from the validation dataloader\n","data_module.show_random_images(val_loader, 'Validation Random Images', num_images=8)\n","\n","# Show random images from the test dataloader\n","data_module.show_random_images(test_loader, 'Test Random Images', num_images=8)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T16:39:23.933432Z","iopub.status.busy":"2024-05-30T16:39:23.933044Z","iopub.status.idle":"2024-05-30T16:39:24.127787Z","shell.execute_reply":"2024-05-30T16:39:24.126673Z","shell.execute_reply.started":"2024-05-30T16:39:23.933401Z"},"trusted":true},"outputs":[],"source":["data_dir=os.path.join('/kaggle/input/mvtec-ad')\n","datamod=MVTecDataModule(data_dir=data_dir,batch_size=4,num_workers=4,train_val_split=0.8,shuffle=True,pin_memory=True,image_size=256,normalize=None)\n","datamod.setup()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T15:40:29.411573Z","iopub.status.busy":"2024-05-30T15:40:29.411166Z","iopub.status.idle":"2024-05-30T15:40:29.421190Z","shell.execute_reply":"2024-05-30T15:40:29.420105Z","shell.execute_reply.started":"2024-05-30T15:40:29.411534Z"},"trusted":true},"outputs":[],"source":["#Algorithm defined in PatchCore paper\n","def obtainCoreset(\n","        memory_bank,\n","        l = 1000, \n","        eps = 0.5,\n","        ):\n","    \n","    #random linear projection in lower dimension (more details in citation 49)\n","    memory_bank = memory_bank.detach().cpu().numpy()\n","    transformer = random_projection.SparseRandomProjection(eps=eps)\n","    memory_bank = torch.tensor(transformer.fit_transform(memory_bank))\n","    last_val = memory_bank[0]\n","    coreset_indices = [0]\n","    min_distances = torch.linalg.norm(memory_bank - last_val, dim= 1, keepdims = True)\n","\n","    for _ in tqdm(range(l - 1)):\n","        distances = torch.linalg.norm(memory_bank - last_val, dim = 1, keepdims = True)\n","        min_distances = torch.minimum(distances, min_distances)\n","        index = torch.argmax(min_distances).item()       \n","        last_val = memory_bank[index]\n","        min_distances[index] = 0\n","        coreset_indices.append(index)\n","    return torch.tensor(coreset_indices)\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T15:40:29.422748Z","iopub.status.busy":"2024-05-30T15:40:29.422460Z","iopub.status.idle":"2024-05-30T15:40:29.439920Z","shell.execute_reply":"2024-05-30T15:40:29.438671Z","shell.execute_reply.started":"2024-05-30T15:40:29.422724Z"},"trusted":true},"outputs":[],"source":["def relaxedContrastiveLoss(w, delta, m): \n","    return torch.mean(w * delta**2 + (1 - w) * torch.max(m - delta, torch.tensor(0.0))**2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T15:40:29.444502Z","iopub.status.busy":"2024-05-30T15:40:29.444089Z","iopub.status.idle":"2024-05-30T15:40:29.481471Z","shell.execute_reply":"2024-05-30T15:40:29.480150Z","shell.execute_reply.started":"2024-05-30T15:40:29.444465Z"},"trusted":true},"outputs":[],"source":["#We set \\sigma = 1 based on the cited papers 18 and 19 in ReConPatch\n","class ContextualSimilarity(nn.Module):\n","    def __init__(self, k = 5, alpha = 0.5):\n","        super(ContextualSimilarity, self).__init__()\n","        self.k = k\n","        self.alpha = alpha\n","        \n","    def forward(self, z):\n","        distances = torch.cdist(z,z)  \n","        topk_dist = -torch.topk(-distances, self.k)[0][:,-1]\n","        filtering = (distances <= topk_dist.unsqueeze(-1)).float()\n","        similarity = torch.matmul(filtering, filtering.transpose(0, 1)) / torch.sum(filtering, dim=-1, keepdim=True)\n","        R = filtering * filtering.transpose(0, 1)\n","        similarity = torch.matmul(similarity, R.transpose(0, 1)) / torch.sum(R, dim=-1, keepdim=True)\n","        return self.alpha * (similarity + similarity.transpose(0, 1))\n","        \n","        \n","class PairwiseSimilarity(nn.Module):\n","    def __init__(self, sigma = 1.0):\n","        super(PairwiseSimilarity, self).__init__()\n","        self.sigma = sigma\n","    def forward(self,z):\n","        return torch.exp(-(torch.cdist(z,z)/self.sigma))\n","\n","\n","class ReConPatch(LightningModule):\n","    def __init__(\n","        self,\n","        input_dim = 1536,\n","        emb_dim = 512,\n","        proj_dim = 1024,\n","        alpha = 0.5,\n","        margin=0.1,\n","        \n","    ):  \n","        super(ReConPatch, self).__init__() \n","        self.memory_bank = []\n","        self.memory_bank_sample = []\n","        \n","        self.fmap =[]\n","        \n","        #self.auroc = AUROC(task=\"multiclass\", num_classes=15)\n","        self.margin = margin\n","        self.alpha = alpha\n","\n","        self.frac_rate= 0.01\n","        self.eps_coreset = 0.5\n","        self.wr_model = torch.hub.load('pytorch/vision:v0.13.0', 'wide_resnet50_2', pretrained=True)\n","        \n","        def hook(module, input, output):\n","            self.fmap.append(output)\n","        self.wr_model.layer2.register_forward_hook(hook)            \n","        self.wr_model.layer3.register_forward_hook(hook)\n","        #setup network 1\n","        self.repr_layer = nn.Linear(input_dim, emb_dim)\n","        self.proj_layer = nn.Linear(emb_dim, proj_dim)\n","        \n","        #setup network 2\n","        self.repr_layer_2=nn.Linear(input_dim, emb_dim)\n","        self.proj_layer_2=nn.Linear(emb_dim, proj_dim)\n","        self.ema_repr = EMA(self.repr_layer_2)\n","        self.ema_proj = EMA(self.proj_layer_2) \n","        #???\n","        with torch.no_grad():\n","            self.proj_layer.weight.copy_(torch.randn_like(self.proj_layer.weight))\n","            self.proj_layer.bias.copy_(torch.randn_like(self.proj_layer.bias))\n","            self.repr_layer.weight.copy_(torch.randn_like(self.repr_layer.weight))\n","            self.repr_layer.bias.copy_(torch.randn_like(self.repr_layer.bias))\n","        self.ema_repr.update()\n","        self.ema_proj.update()\n","        \n","        self.pairwise_sim=PairwiseSimilarity()\n","        self.contextual_sim=ContextualSimilarity()\n","    def forward(self, x):\n","        '''\n","        x has dimensions B x C x H x W (batch channels height width)\n","        '''\n","        #-----------FROM PRETRAINED MODEL TO FEATURE MAP\n","        self.fmap = []\n","        self.memory_bank_sample = []\n","        \n","        y = self.wr_model(x)\n","        \n","        #NOTE: We are taking as dimensions the ones of the feature map with higher resolution as specified in\n","        #chapter 3.1 of the patchcore paper (https://arxiv.org/pdf/2106.08265)\n","        dimensions = (\n","                int(torch.Tensor([t.shape[-2] for t in self.fmap]).max().item()),\n","                int(torch.Tensor([t.shape[-1] for t in self.fmap]).max().item())\n","            )\n","        \n","        blur = nn.AvgPool2d(3, stride = 1)\n","        resizer = nn.AdaptiveAvgPool2d(dimensions)\n","        preprocess = lambda t : resizer(blur(t))\n","        feature_stacks = torch.cat([preprocess(m) for m in self.fmap], dim=1)\n","        \n","        \n","        #-----------RECONPATCH\n","        feature_stacks_reshaped = feature_stacks.reshape(feature_stacks.shape[1], -1).T\n","        #feature_stacks_reshaped_cpu = feature_stacks_reshaped.detach().cpu().numpy()\n","        self.memory_bank_sample.append(feature_stacks_reshaped)\n","        \n","        \n","        #----------network1 pass\n","        h1=self.ema_repr(feature_stacks_reshaped)\n","        z1=self.ema_proj(h1)\n","        \n","        p_sim=self.pairwise_sim(z1)\n","        c_sim=self.contextual_sim(z1)\n","        \n","        w=self.alpha*p_sim+(1-self.alpha)*c_sim\n","        #----------network2 pass\n","        h2=self.repr_layer_2(feature_stacks_reshaped)\n","        z2=self.proj_layer_2(h2)\n","        \n","        \n","        pairwise_distances = torch.cdist(z2, z2, p=2)\n","\n","        # Add a small epsilon to avoid division by zero when taking square root\n","        epsilon = 1e-9\n","        distances = torch.sqrt(pairwise_distances + epsilon)\n","        delta=distances/torch.mean(distances)\n","        \n","        return w ,delta\n","         \n","    def training_step(self, batch, batch_idx):\n","        x, y = batch\n","        w,delta = self(x)\n","        loss = relaxedContrastiveLoss(w, delta, self.margin)\n","        \n","        if self.frac_rate < 1:\n","            \n","            self.memory_bank_sample = torch.cat(self.memory_bank_sample, axis = 0)\n","            if self.memory_bank == []:\n","                self.memory_bank = self.memory_bank_sample\n","                mem_concat = self.memory_bank\n","            else:\n","\n","                mem_concat = torch.cat([self.memory_bank,self.memory_bank_sample], axis = 0)\n","            \n","            num_coreset_points = int(self.frac_rate * mem_concat.shape[0])\n","            coreset_indices = obtainCoreset(mem_concat, l = num_coreset_points, eps = self.eps_coreset)\n","            self.memory_bank = mem_concat[coreset_indices]            \n","        self.log('train_loss', loss)\n","        return loss\n","\n","    def configure_optimizers(self):\n","        optimizer = AdamP(self.parameters(), lr=1e-6, betas=(0.9, 0.999), weight_decay=1e-2)\n","        scheduler = CosineAnnealingLR(optimizer, T_max=10)  \n","        return [optimizer], [scheduler]\n","\n","    def validation_step(self, batch, batch_idx):\n","        x, y = batch\n","        w,delta = self(x)\n","        loss = relaxedContrastiveLoss(w, delta, self.margin)\n","        self.log('val_loss', loss)\n","        return loss\n","\n","    def test_step(self, batch, batch_idx):\n","        x, y = batch\n","        w,delta = self(x)\n","        loss = relaxedContrastiveLoss(w, delta, self.margin)\n","        self.log('test_loss', loss)\n","        return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T15:40:29.483435Z","iopub.status.busy":"2024-05-30T15:40:29.482979Z","iopub.status.idle":"2024-05-30T16:02:28.906329Z","shell.execute_reply":"2024-05-30T16:02:28.905386Z","shell.execute_reply.started":"2024-05-30T15:40:29.483404Z"},"trusted":true},"outputs":[],"source":["wandb.login(key = '4e6b8c4cffac1d8cf8ca1a28ff5ef5d77333d2de')\n","wandb.init(project='ReConPatch')\n","wandb_logger = WandbLogger(project='ReConPatch')\n","                                     \n","reconpatch = ReConPatch()\n","trainer = pl.Trainer(\n","    max_epochs=5, \n","    logger=wandb_logger, \n","    callbacks=[\n","        torchpl.callbacks.ModelCheckpoint(\n","            monitor='val_loss', \n","            save_top_k=1, \n","            mode='min',\n","            save_last=True\n","        ),\n","        torchpl.callbacks.EarlyStopping(\n","            monitor=\"val_loss\",\n","            patience=10,\n","            mode=\"min\"),\n","      ], \n","    devices=[0],\n",")\n","trainer.fit(model=reconpatch, datamodule=datamod)\n","\n","#close the wandb session\n","wandb.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T16:02:28.908118Z","iopub.status.busy":"2024-05-30T16:02:28.907780Z","iopub.status.idle":"2024-05-30T16:02:28.913080Z","shell.execute_reply":"2024-05-30T16:02:28.911935Z","shell.execute_reply.started":"2024-05-30T16:02:28.908085Z"},"trusted":true},"outputs":[],"source":["\n","#reconpatch = ReConPatch.load_from_checkpoint(\"/kaggle/input/checkpoint/epoch26-step9801.ckpt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T16:02:28.915048Z","iopub.status.busy":"2024-05-30T16:02:28.914625Z","iopub.status.idle":"2024-05-30T16:02:28.927558Z","shell.execute_reply":"2024-05-30T16:02:28.926349Z","shell.execute_reply.started":"2024-05-30T16:02:28.915011Z"},"trusted":true},"outputs":[],"source":["mem_bank = reconpatch.memory_bank\n","mem_bank = torch.tensor(mem_bank)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T16:02:28.929930Z","iopub.status.busy":"2024-05-30T16:02:28.929011Z","iopub.status.idle":"2024-05-30T16:02:28.934987Z","shell.execute_reply":"2024-05-30T16:02:28.934120Z","shell.execute_reply.started":"2024-05-30T16:02:28.929892Z"},"trusted":true},"outputs":[],"source":["#reconpatch = ReConPatch()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T16:02:28.936562Z","iopub.status.busy":"2024-05-30T16:02:28.936257Z","iopub.status.idle":"2024-05-30T16:02:28.944520Z","shell.execute_reply":"2024-05-30T16:02:28.943563Z","shell.execute_reply.started":"2024-05-30T16:02:28.936533Z"},"trusted":true},"outputs":[],"source":["# sample, target = next(iter(datamod.train_dataloader()))\n","# with torch.no_grad():\n","#     y = reconpatch(sample)\n","#     mem_bank = reconpatch.memory_bank"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T16:02:28.946009Z","iopub.status.busy":"2024-05-30T16:02:28.945675Z","iopub.status.idle":"2024-05-30T16:02:29.038989Z","shell.execute_reply":"2024-05-30T16:02:29.037980Z","shell.execute_reply.started":"2024-05-30T16:02:28.945984Z"},"trusted":true},"outputs":[],"source":["print(mem_bank)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T16:02:29.040862Z","iopub.status.busy":"2024-05-30T16:02:29.040527Z","iopub.status.idle":"2024-05-30T16:02:29.045955Z","shell.execute_reply":"2024-05-30T16:02:29.044829Z","shell.execute_reply.started":"2024-05-30T16:02:29.040835Z"},"trusted":true},"outputs":[],"source":["# feature_dim=len(reconpatch.fmap)\n","# l=len(datamod.train_dataloader())\n","\n","# #memory_bank=torch.zeros(l,6272,1536)#dataloader_size,feature_reshaped_size\n","# for i, values in enumerate(tqdm(datamod.train_dataloader())): \n","#     img,label=values\n","#     y = reconpatch(img)\n","#     feature_map = reconpatch.fmap\n","# #     print(feature_map)\n","#     dimensions = (\n","#                 int(torch.Tensor([t.shape[-2] for t in feature_map]).max().item()),\n","#                 int(torch.Tensor([t.shape[-1] for t in feature_map]).max().item())\n","#     )\n","#     blur = nn.AvgPool2d(3, stride = 1)\n","#     resizer = nn.AdaptiveAvgPool2d(feature_map[0].shape[-2])\n","#     preprocess = lambda t : resizer(blur(t))\n","#     feature_stacks = torch.cat([preprocess(m) for m in feature_map], dim=1)\n","#     feature_stacks_reshaped = feature_stacks.reshape(feature_stacks.shape[1], -1).T\n","#     print(feature_stacks_reshaped.shape)\n","#     break\n","#     memory_bank[i]=feature_stacks_reshaped"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T16:02:29.047418Z","iopub.status.busy":"2024-05-30T16:02:29.047093Z","iopub.status.idle":"2024-05-30T16:02:29.056359Z","shell.execute_reply":"2024-05-30T16:02:29.055355Z","shell.execute_reply.started":"2024-05-30T16:02:29.047392Z"},"trusted":true},"outputs":[],"source":["# memory_bank = np.concatenate(memory_bank, axis = 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T16:02:29.058115Z","iopub.status.busy":"2024-05-30T16:02:29.057747Z","iopub.status.idle":"2024-05-30T16:02:29.066566Z","shell.execute_reply":"2024-05-30T16:02:29.065720Z","shell.execute_reply.started":"2024-05-30T16:02:29.058081Z"},"trusted":true},"outputs":[],"source":["# frac_rate= 0.01\n","# eps_coreset = 0.05\n","# if frac_rate < 1:\n","#     num_coreset_points = int(frac_rate * len(memory_bank))\n","#     coreset_indices = obtainCoreset(mem_bank, l=num_coreset_points, eps=eps_coreset)\n","#     mem_bankory = mem_bankory[coreset_indices]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T16:27:03.975832Z","iopub.status.busy":"2024-05-30T16:27:03.974951Z","iopub.status.idle":"2024-05-30T16:27:03.993440Z","shell.execute_reply":"2024-05-30T16:27:03.992255Z","shell.execute_reply.started":"2024-05-30T16:27:03.975796Z"},"trusted":true},"outputs":[],"source":["def anomalyDetection(batch_images):\n","    # Check if batch_images is a batch or a single image\n","    if batch_images.dim() == 3:\n","        batch_images = batch_images.unsqueeze(0)\n","        \n","    batch_size = batch_images.size(0)\n","    scores = []\n","\n","    for i in range(batch_size):\n","        image = batch_images[i]\n","\n","        feature_map = reconpatch.fmap\n","        dimensions = (\n","            int(torch.Tensor([t.shape[-2] for t in feature_map]).max().item()),\n","            int(torch.Tensor([t.shape[-1] for t in feature_map]).max().item())\n","        )\n","\n","        blur = nn.AvgPool2d(3, stride=1)\n","        resizer = nn.AdaptiveAvgPool2d(dimensions)\n","        preprocess = lambda t: resizer(blur(t))\n","        feature_stacks = torch.cat([preprocess(m) for m in feature_map], dim=1)\n","        feature_stacks_reshaped = feature_stacks.reshape(feature_stacks.shape[1], -1).T\n","\n","        distances = torch.cdist(feature_stacks_reshaped, mem_bank, p=2)\n","        dist_val, dist_val_idxs = torch.min(distances, dim=1)\n","        s_idx = torch.argmax(dist_val)\n","        s_star = torch.max(dist_val)\n","        m_test_star = torch.unsqueeze(feature_stacks_reshaped[s_idx], dim=0)\n","        m_star = mem_bank[dist_val_idxs[s_idx]].unsqueeze(0)\n","\n","        knn_dists = torch.cdist(m_star, mem_bank, p=2.0)\n","        _, nn_idxs = knn_dists.topk(k=3, largest=False)\n","\n","        m_star_neighbourhood = mem_bank[nn_idxs[0, 1:]]\n","        w_denominator = torch.linalg.norm(m_test_star - m_star_neighbourhood, dim=1)\n","        norm = torch.sqrt(torch.tensor(feature_stacks_reshaped.shape[1]))\n","        w = 1 - (torch.exp(s_star / norm) / torch.sum(torch.exp(w_denominator / norm)))\n","        s = w * s_star\n","\n","        scores.append(s)\n","\n","    return torch.stack(scores)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T16:39:59.857224Z","iopub.status.busy":"2024-05-30T16:39:59.856801Z","iopub.status.idle":"2024-05-30T16:40:31.941075Z","shell.execute_reply":"2024-05-30T16:40:31.939671Z","shell.execute_reply.started":"2024-05-30T16:39:59.857189Z"},"trusted":true},"outputs":[],"source":["\n","img_list=[]\n","labels_list=[]\n","for img,label in tqdm(datamod.test_dataloader()):\n","    #print(type(img[0][0][0][0].item()))\n","    labels_list.append(label)\n","   \n","    probs = anomalyDetection(img)\n","    probs=probs.to('cpu')\n","    img_list.append(probs)\n","\n","\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T16:29:41.854173Z","iopub.status.busy":"2024-05-30T16:29:41.853730Z","iopub.status.idle":"2024-05-30T16:29:41.862231Z","shell.execute_reply":"2024-05-30T16:29:41.861102Z","shell.execute_reply.started":"2024-05-30T16:29:41.854135Z"},"trusted":true},"outputs":[],"source":["print(img_list[400])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T16:32:16.684789Z","iopub.status.busy":"2024-05-30T16:32:16.683681Z","iopub.status.idle":"2024-05-30T16:32:17.035945Z","shell.execute_reply":"2024-05-30T16:32:17.034509Z","shell.execute_reply.started":"2024-05-30T16:32:16.684742Z"},"trusted":true},"outputs":[],"source":["from torchmetrics import AUROC\n","print('img_list',len(img_list))\n","print('label_list_shape',len(labels_list))\n","#labels_list= torch.cat(labels_list,dim=0)\n","#img_list= torch.cat(img_list,dim=0)\n","auroc = AUROC(task=\"multiclass\", num_classes=15)\n","auroc.update(img_list, labels_list)\n","auroc_value = auroc.compute()\n","print(auroc_value.item())"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":1485541,"sourceId":2454327,"sourceType":"datasetVersion"},{"datasetId":1946896,"sourceId":3209332,"sourceType":"datasetVersion"},{"datasetId":5042808,"sourceId":8459939,"sourceType":"datasetVersion"},{"datasetId":5057748,"sourceId":8480003,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":4}
